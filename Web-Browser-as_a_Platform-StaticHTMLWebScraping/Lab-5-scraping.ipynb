{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab and Homework 5\n",
        "## The Web as a Platform I: HTML and Web Scraping\n",
        "\n",
        "**In class**: *Thursday, March 5, 2020*  \n",
        "**Homework Due** : *5 PM, Thursday, March 12, 2020*\n",
        "\n",
        "# Learning Goals\n",
        "\n",
        "To date we've covered the backend design of a distributed architecture: databases, RPC, REST APIs, etc. This week and next we switch to the frontend. Specifically, we will learn about the a suite of technologies such as HTML, the DOM and Javascript that make the web browser a ubiquitous platform. \n",
        "\n",
        "The focus this week is on HTML and tools for parsing and displaying web content. We'll discuss browser architecture and the DOM in class but the emphasis in this lab is on scraping web content to extract and process data.\n",
        "\n",
        "The goals are:\n",
        "\n",
        "* Learn about HTML and its syntactic structure.\n",
        "* Learn about the utility, tools and techniques of data scraping.\n",
        "* Learn how HTML files are parsed into a form that is amenable to easy analysis and extraction.\n",
        "* Brush up on pandas Dataframes, particularly indexing and aggregation.\n",
        "\n",
        "This notebook consists of two parts. \n",
        "\n",
        "The first part introduces basic concepts. We'll explore these concepts during class. The second part is for homework, which builds upon and extends the things you learned in class.\n",
        "\n",
        "# Web Scraping\n",
        "\n",
        "Acquiring data is the first step to doing anything useful in Data Science.\n",
        "\n",
        "Unfortunately, the required data often isn't readily available in an easy-to-read zipfile or database, ready to be exploited. You generally have to find it, get it and shape it to your needs.\n",
        "\n",
        "Fortunately, the web is a rich source of information and we will use we scraping to the get data we need. \n",
        "\n",
        "# Problem Statement\n",
        "\n",
        "Let's say you're doing research on corporate governance in the Fortune 100 and want to determine compensation rates of executives and their potential conflicts. \n",
        "\n",
        "How might you go about getting the data for the companies in the list? Perhaps if you have a subscription to Bloomberg, then you could download a CVS. But if you don't, your best bet is scrape public pages on the web.\n",
        "\n",
        "In this lab and homework we'll learn how to scrape Yahoo Finance pages to create our own data set ready for analysis. We'll do the scraping in class and the analysis for homework.\n",
        "\n",
        "# Scraping Summarized\n",
        "\n",
        "In principle, web scraping is simple and involves the following steps:\n",
        "\n",
        "1. **Inspect** the web page. This will give you a sense the overall structure of the page and where the relevant information resides. To do this, you will open the web page in a browser and then view the page *source*. We explain how below.\n",
        "\n",
        "1. **Retrieve** a web page over HTTP as text. This text will be formatted as HTML, the language of the World Wide Web. Your browser interprets the content of an HTML file and renders it to screen. (This is a gross over-simplification. Many sites today are *dynamic* apps written in Javascript. They retrieve data over an API and render it programmatically on screen. Still, a lot of information resides in *static* HTML and the techniques described here remain quite useful.)\n",
        "\n",
        "1. **Parse** the retrieved HTML text into a form that can easily scanned and operated upon. Fortunately, we don't have to do this ourselves. Great and powerful Python libraries such as [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) and [Scrapy](https://scrapy.org) already exist. In this unit, we'll use Beautiful Soup. But I encourage you to take a look at Scrapy. It's in many ways more powerful and flexible.\n",
        "\n",
        "1. **Search** the parsed HTML for the information we're interested in.\n",
        "\n",
        "1. **Pull** the relevant data out of the HTML, reformat it into a form that meets our needs, and save it for later analysis. In this lab, we'll insert our data into a [Pandas](https://pandas.pydata.org) Dataframe. In future units, we'll likely save to a database.\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I: In Class Lab\n",
        "\n",
        "For the in-class part of this unit, we are going to do the following:\n",
        "\n",
        "1. Retrieve the stock symbols for the companies in the S&P 100. To do this, we'll scrape a Wikipedia page.\n",
        "2. With the S&P symbols in hand, we'll retrieve the board of directors by scraping a page for each company on the Reuters Financial site. You will use the skills developed in scraping the Wikipedia page to get the boards.\n",
        "3.You will next use your Pandas chops to compute the average age of each company board. Easy Peasy!\n",
        "\n",
        "## Imports\n",
        "\n",
        "These are the packages we'll use in this lab. Please use the [Installation]() notebook to set up all the modules we'll be using this semester. We'll add new packages in the Installation notebook as they are needed."
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Packages to install\n",
        "\n",
        "# pretty printer\n",
        "import pprint\n",
        "\n",
        "# set up the pretty printer\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "# BeautifulSoup for scraping\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# for making HTTP requests\n",
        "import requests\n",
        "\n",
        "# Pandas/numpy for data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2020-03-05T15:18:51.903Z",
          "iopub.execute_input": "2020-03-05T15:18:51.917Z",
          "iopub.status.idle": "2020-03-05T15:18:53.053Z",
          "shell.execute_reply": "2020-03-05T15:18:53.061Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve the S&P 100 Stock Symbols\n",
        "\n",
        "In order to investigate the board of directors at each company in the S&P 100, we're going to need their stock symbols. Here we'll scrape the [Wikipedia page](https://en.wikipedia.org/wiki/S%26P_100) for the S&P 100.\n",
        "\n",
        "You should load the [Wiki](https://en.wikipedia.org/wiki/S%26P_100) page in a browser and study it's HTML source. (One easy way to do this is to use the developer tools built into your browser. See [here](https://developers.google.com/web/ilt/pwa/tools-for-pwa-developers) for instructions on opening the developer console on your particular browser.)\n",
        "\n",
        "The listing of S&P companies is in a `<table>` element. We'll use BeautifulSoup to parse the HTML into a parse tree, a hierarchical representation of the page, which makes it much easier to scan for the elements we want. BeautifulSoup will scan the parse tree, find the table in question, and scan each row for the company and symbol. We'll put the extracted data in a Pandas DataFrame for later analysis and manipulation."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# The URL for the Wikipedia page we're scraping\n",
        "WIKI_URL = 'https://en.wikipedia.org/wiki/S%26P_100'\n",
        "\n",
        "# Retrieve the page\n",
        "wiki_page = requests.get(WIKI_URL).text\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false,
        "execution": {
          "iopub.status.busy": "2020-03-05T15:19:26.012Z",
          "iopub.execute_input": "2020-03-05T15:19:26.018Z",
          "iopub.status.idle": "2020-03-05T15:19:26.144Z",
          "shell.execute_reply": "2020-03-05T15:19:26.156Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have the wiki page in the variable `wiki_page`. We should print it directly the output will be unstructured and therefore quite hard to read.\n",
        "\n",
        "Let's use Beautiful Soup to read the text into a parse tree and then render the parse tree to screen like this:"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parse the HTML text into a tree\n",
        "soup = BeautifulSoup(wiki_page, 'html.parser')\n",
        "\n",
        "# print the tree to screen\n",
        "print(soup.prettify())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false,
        "execution": {
          "iopub.status.busy": "2020-03-05T15:19:38.063Z",
          "iopub.execute_input": "2020-03-05T15:19:38.070Z",
          "iopub.status.idle": "2020-03-05T15:19:38.180Z",
          "shell.execute_reply": "2020-03-05T15:19:38.337Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whoa!!! There's a lot going on here. \n",
        "\n",
        "But if as you poke around, you find that the ticker symbols are in an HTML `<table>`:\n",
        "\n",
        "```erb\n",
        "<table class=\"wikitable sortable\">\n",
        "       <tbody>\n",
        "        <tr>\n",
        "         <th>\n",
        "          Symbol\n",
        "         </th>\n",
        "         <th>\n",
        "          Name\n",
        "         </th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "         <td>\n",
        "          AAPL\n",
        "         </td>\n",
        "         <td>\n",
        "          <a href=\"/wiki/Apple_Inc.\" title=\"Apple Inc.\">\n",
        "           Apple Inc.\n",
        "          </a>\n",
        "         </td>\n",
        "        </tr>\n",
        "```\n",
        "\n",
        "The body of the table has two columns. The symbol is in the left column and the company name on the right. \n",
        "\n",
        "Let's use BeautifulSoup to extract the table from the parse tree. \n",
        "\n",
        "We're looking for the table with CSS classes `wikitable` and `sortable`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the table containin the S&P companies\n",
        "sandp_table = soup.find('table', {\"class\" : \"wikitable sortable\"})\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false,
        "execution": {
          "iopub.status.busy": "2020-03-05T15:20:41.620Z",
          "iopub.execute_input": "2020-03-05T15:20:41.629Z",
          "iopub.status.idle": "2020-03-05T15:20:41.642Z",
          "shell.execute_reply": "2020-03-05T15:20:41.651Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our table of S&P companies in hand, we can traverse it row by row to retrieve each company and its symbol. Here's how:"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# snps array will hold an array of tuples of the form (Symbol, Name)\n",
        "snps = []\n",
        "\n",
        "# scan the table for each row ('tr' is the HTML tag for a table row)\n",
        "for row in sandp_table.find_all('tr'):\n",
        "    \n",
        "    # scan the row for table cells ('td' is the tag for table data)\n",
        "    cols = row.find_all('td')\n",
        "    \n",
        "    if len(cols) == 2: # skip the header row\n",
        "        snps.append((cols[0].text.strip(), cols[1].text.strip()))\n",
        "\n",
        "# convert the array of tuples into a Pandas DataFrame        \n",
        "snps_df = pd.DataFrame(snps, columns=['Symbol', 'Name'])\n",
        "\n",
        "snps_df\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false,
        "execution": {
          "iopub.status.busy": "2020-03-05T15:20:47.062Z",
          "iopub.execute_input": "2020-03-05T15:20:47.069Z",
          "iopub.status.idle": "2020-03-05T15:20:47.096Z",
          "shell.execute_reply": "2020-03-05T15:20:47.233Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Problem 1\n",
        "\n",
        "We now have the S&P 100 stock symbols. But we're far from done. What we want is information about the Executives of each company, which we'll do in this Lab.\n",
        "\n",
        "To do this, iterate over the snps_df DataFrame created above and scrape the Yahoo Financials page for company boards.\n",
        "\n",
        "As one example, here's the Yahoo Finance page for [Apple](https://finance.yahoo.com/quote/AAPL/profile?p=AAPL) (AAPL). Open the page in a browser and study the HTML source to find where the Key Executive information is located. Then flesh out the skeleton code below.\n",
        "\n",
        "Organize each row of the table into the following columns:\n",
        "\n",
        "1. **Symbol**: the ticker symbol of the company.\n",
        "1. **Name**: the name of the executive.\n",
        "1. **Title**: Title and role\n",
        "1. **Pay**: Compensation, usually in Millions\n",
        "1. **Age**: Executive age encoded as an `integer`. (We want to run aggregate functions on the age). Yahoo gives the data of birth; you will have to convert to an age.\n",
        "\n",
        "Create a DataFrame called `df` with the above column names."
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BASE_URL = 'https://finance.yahoo.com/quote/DIS/profile?p=AAPL'\n",
        "\n",
        "symbol_array = snps_df['Symbol'].values\n",
        "\n",
        "# board_members will hold an array of tuples, one for each board member\n",
        "execs = []\n",
        "\n",
        "# for simplicity only look at the first five companies in class\n",
        "for sym in symbol_array[:5]:\n",
        "# for (index, co) in snps_df.iterrows():\n",
        "#    sym = co['Symbol']\n",
        "    \n",
        "# df = pd.DataFrame(execs, ...)\n",
        "\n",
        "# return and print df\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Problem 2\n",
        "\n",
        "Notice that the `Symbol` column in the `df` DataFrame above has multiple rows for each company--one row for each board member.\n",
        "\n",
        "We want to retieve and aggregate over individual companies. One way to do this is to create a Pandas *multiindex* on `Symbol` and `Name`. Create an `inplace` index, i.e., one that modifies the `df` DataFrame instead of returning a new one."
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab Problem 2\n",
        "# Create a multiindex as described above\n",
        "\n",
        "df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Problem 3\n",
        "\n",
        "Use the indexed `df` to retrieve the entries for `GOOG`.\n",
        "\n",
        "Hint: use `df.loc`"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab problem 3\n",
        "\n",
        "# your code here"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Problem 4\n",
        "\n",
        "Compute the mean age of each executive team in the S&P 100. Which company has the oldest board? Which the youngest?"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab problem 4\n",
        "# compute and print the mean age of each board\n",
        "# your code here"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the boards with the maximum and minimum average ages"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework\n",
        "\n",
        "*40 points total*\n",
        "\n",
        "In the homework you will build upon the lab work to retrieve data about each executive and then perform some basic aggregations.\n",
        "\n",
        "## Homework Problem 1\n",
        "\n",
        "*20 points*\n",
        "\n",
        "Retrieve the total compensation of each executive and put the results in a DataFrame of the following columns:\n",
        "\n",
        "1. **Symbol**: The company stock symbol.\n",
        "1. **Name**: Executive name.\n",
        "1. **Total**: Total yearly compensation for the member.\n",
        "\n",
        "To do this problem you'll use the `link`attribute in the `df` DataFrame from the lab.\n",
        "\n",
        "You should decide how to index the DataFrame to best utilize it for subsequent problems."
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Homework problem 1\n",
        "compensation_table = None;\n",
        "\n",
        "# Your code here\n",
        "\n",
        "compensation_table"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework Problem 2\n",
        "\n",
        "*20 points*\n",
        "\n",
        "Compute the mean compensation for each company and put the results in a DataFrame with the following columns:\n",
        "\n",
        "1. **Symbol**: The company stock symbol\n",
        "1. **Compensation**: Mean executive compensation\n",
        "1. **Age**: Mean executive age\n",
        "\n",
        "Notice that you're asked to include the mean age. This suggests that you will `join` two tables together. How will you index these two tables in order to compute the result elegantly and simply?"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Homework problem 2\n",
        "company_compansation_table = None\n",
        "\n",
        "# Your code here\n",
        "\n",
        "company_compensation_table"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.22.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}